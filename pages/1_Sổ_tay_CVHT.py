# pages/1_S·ªï_tay_CVHT.py
import re
import unicodedata
import requests
import streamlit as st
import fitz  # PyMuPDF
from dataclasses import dataclass
from typing import List, Dict, Union, Tuple

# Import c√°c th∆∞ vi·ªán c·∫ßn thi·∫øt cho vi·ªác x·ª≠ l√Ω machine learning
from sklearn.feature_extraction.text import TfidfVectorizer
from sklearn.metrics.pairwise import cosine_similarity

# ============== C·∫•u h√¨nh m·∫∑c ƒë·ªãnh ==============
MANIFEST_URL_DEFAULT = "https://raw.githubusercontent.com/thanvanvan82/cvht-streamlit-app/main/manifest.json"

# ===================== C√°c h√†m ti·ªán √≠ch (gi·ªØ nguy√™n kh√¥ng ƒë·ªïi) =====================
@st.cache_data(show_spinner="ƒêang t·∫£i manifest...", ttl=3600)
def fetch_manifest(url: str) -> List[Dict[str, str]]:
    try:
        r = requests.get(url, timeout=15)
        r.raise_for_status()
        return r.json()
    except Exception as e:
        st.error(f"Kh√¥ng th·ªÉ t·∫£i manifest: {e}")
        return []

@st.cache_data(show_spinner=False, ttl=3600, max_entries=128)
def download_pdf(url: str) -> bytes:
    headers = {"User-Agent": "st-pdf-app/1.0"}
    r = requests.get(url, headers=headers, timeout=60)
    r.raise_for_status()
    return r.content

def pdf_to_pages(pdf_bytes: bytes) -> List[str]:
    pages = []
    with fitz.open(stream=pdf_bytes, filetype="pdf") as doc:
        for page in doc:
            text = page.get_text("text")
            normalized_text = re.sub(r"-\s*\n\s*", "", text)
            normalized_text = re.sub(r"[ \t]+", " ", normalized_text.replace("\u00ad", "").replace("\u00A0", " "))
            normalized_text = re.sub(r"\s*\n\s*", "\n", normalized_text).strip()
            pages.append(normalized_text)
    return pages

@dataclass
class Chunk:
    doc_name: str
    url: Union[str, None]
    page: int
    text: str

def build_chunks(pages: List[str], doc_name: str, url: Union[str, None], chunk_size=1100, overlap=150) -> List[Chunk]:
    chunks: List[Chunk] = []
    for i, page_text in enumerate(pages):
        if not page_text: continue
        start, L = 0, len(page_text)
        while start < L:
            end = min(L, start + chunk_size)
            txt = page_text[start:end].strip()
            if txt:
                chunks.append(Chunk(doc_name=doc_name, url=url, page=i + 1, text=txt))
            new_start = end - overlap
            start = new_start if new_start > start else end
    return chunks

@st.cache_resource(show_spinner="ƒêang t·∫°o ch·ªâ m·ª•c...")
def make_index(_chunks_tuple: tuple):
    chunks = list(_chunks_tuple)
    corpus = [c.text.lower() for c in chunks]
    vec = TfidfVectorizer(analyzer="word", ngram_range=(1, 2), token_pattern=r"(?u)\b\w+\b", max_df=0.95, min_df=1)
    X = vec.fit_transform(corpus)
    return vec, X

def search(query: str, chunks: List[Chunk], vec, X, remove_diacritics: bool, exact_match: bool, top_k: int) -> List[Tuple[float, float, Chunk]]:
    if not query.strip(): return []
    q = query.strip().lower()
    if remove_diacritics:
        q = "".join(c for c in unicodedata.normalize('NFD', q) if unicodedata.category(c) != 'Mn')
    
    q_vec = vec.transform([q])
    sims = cosine_similarity(q_vec, X).ravel()
    
    candidate_indices = sims.argsort()[::-1][:max(top_k * 5, 100)]
    results = []
    for idx in candidate_indices:
        raw_score = float(sims[idx])
        if raw_score < 0.01: continue
            
        ch = chunks[idx]
        hay = ch.text.lower()
        if remove_diacritics:
            hay = "".join(c for c in unicodedata.normalize('NFD', hay) if unicodedata.category(c) != 'Mn')
        
        key_hit = q in hay
        
        if exact_match and not key_hit:
            continue
            
        boost = 1.0 if key_hit and exact_match else (0.2 if key_hit else 0.0)
        sort_score = raw_score + boost
        results.append((sort_score, raw_score, ch))

    results.sort(key=lambda x: x[0], reverse=True)
    return results[:top_k]

def highlight(text: str, query: str, remove_diacritics: bool, max_len: int = 350):
    if not query: return text[:max_len] + ("..." if len(text) > max_len else "")

    base_text = text.lower()
    base_query = query.lower()
    if remove_diacritics:
        base_text = "".join(c for c in unicodedata.normalize('NFD', base_text) if unicodedata.category(c) != 'Mn')
        base_query = "".join(c for c in unicodedata.normalize('NFD', base_query) if unicodedata.category(c) != 'Mn')

    idx = base_text.find(base_query)
    
    if idx < 0:
        snippet = text[:max_len] + ("..." if len(text) > max_len else "")
    else:
        start = max(0, idx - max_len // 3)
        end = min(len(text), idx + len(base_query) + (max_len * 2 // 3))
        snippet = ("..." if start > 0 else "") + text[start:end] + ("..." if end < len(text) else "")
    
    safe_query = re.escape(query)
    pattern = re.compile(f"({safe_query})", flags=re.IGNORECASE)
    highlighted_snippet = pattern.sub(r"<mark>\1</mark>", snippet)
    
    return highlighted_snippet

# ===================== GIAO DI·ªÜN NG∆Ø·ªúI D√ôNG (UI) =====================
st.title("üìñ S·ªï tay CVHT")

tab_tra_cuu, tab_bieu_mau = st.tabs(["Tra c·ª©u Quy ch·∫ø, Quy ƒë·ªãnh", "Quy tr√¨nh, Bi·ªÉu m·∫´u"])

with tab_tra_cuu:
    # --- S·ª¨A L·ªñI 1: Kh·ªüi t·∫°o t·∫•t c·∫£ c√°c bi·∫øn tr·∫°ng th√°i c·∫ßn thi·∫øt ---
    if 'search_results' not in st.session_state:
        st.session_state.search_results = []
    if 'last_keyword' not in st.session_state:
        st.session_state.last_keyword = ""
    if 'last_remove_diac' not in st.session_state:
        st.session_state.last_remove_diac = True

    # --- Sidebar ---
    with st.sidebar:
        st.title("T√πy ch·ªçn")
        st.subheader("Hi·ªÉn th·ªã & K·ªπ thu·∫≠t")
        top_k = st.slider("S·ªë k·∫øt qu·∫£ hi·ªÉn th·ªã", 5, 50, 20, 1)
        
        with st.expander("T√πy ch·ªçn k·ªπ thu·∫≠t"):
            chunk_size = st.slider("K√≠ch th∆∞·ªõc ƒëo·∫°n (k√Ω t·ª±)", 600, 2200, 1100, 50)
            overlap = st.slider("ƒê·ªô ch·ªìng l·∫•n (k√Ω t·ª±)", 50, 400, 150, 10)

    # --- Giao di·ªán d·ª±a tr√™n tr·∫°ng th√°i ---
    if not st.session_state.search_results:
        manifest = fetch_manifest(MANIFEST_URL_DEFAULT)
        if manifest:
            doc_titles = [item.get('title', item['name']) for item in manifest]
            title_to_item_map = {item.get('title', item['name']): item for item in manifest}
            search_options = ["T·∫•t c·∫£ vƒÉn b·∫£n"] + doc_titles
        else:
            title_to_item_map = {}
            search_options = ["T·∫•t c·∫£ vƒÉn b·∫£n"]

        with st.container(border=True):
            with st.form(key="search_form"):
                st.write("**TRA C·ª®U VƒÇN B·∫¢N**")
                
                col1, col2 = st.columns(2)
                with col1:
                    keyword = st.text_input("T·ª´ kh√≥a", placeholder="Nh·∫≠p t·ª´ kh√≥a b·∫°n mu·ªën t√¨m...")
                with col2:
                    selected_doc_title = st.selectbox("VƒÉn b·∫£n", options=search_options, index=1)

                col3, col4, col5 = st.columns([2, 1, 1])
                with col3:
                    exact_match = st.checkbox("T√¨m ch√≠nh x√°c c·ª•m t·ª´", value=False)
                    remove_diac = st.checkbox("T√¨m kh√¥ng d·∫•u (khuy·∫øn ngh·ªã)", value=True)
                with col4:
                    search_submitted = st.form_submit_button("üîç T√¨m ki·∫øm")
                with col5:
                    reset_submitted = st.form_submit_button("üîÑ Nh·∫≠p l·∫°i")

                with st.expander("VƒÉn b·∫£n b·ªï sung (Upload file PDF ƒë·ªÉ tra c·ª©u t·∫°m th·ªùi)"):
                    uploaded_files = st.file_uploader(
                        "K√©o th·∫£ ho·∫∑c ch·ªçn file", type="pdf",
                        accept_multiple_files=True, label_visibility="collapsed"
                    )
        
        with st.container(border=True):
            st.markdown("<h4 style='text-align: center; color: #004b8d;'>H∆Ø·ªöNG D·∫™N TRA C·ª®U</h4>", unsafe_allow_html=True)
            st.markdown("""
            1.  **T·ª´ kh√≥a:** G√µ n·ªôi dung c·∫ßn t√¨m.
            2.  **VƒÉn b·∫£n:** Ch·ªçn ph·∫°m vi t√¨m ki·∫øm (m·∫∑c ƒë·ªãnh l√† vƒÉn b·∫£n ƒë·∫ßu ti√™n).
            3.  **VƒÉn b·∫£n b·ªï sung:** C√≥ th·ªÉ t·∫£i l√™n file PDF c·ªßa ri√™ng b·∫°n ƒë·ªÉ t√¨m ki·∫øm c√πng l√∫c.
            4.  **T√πy ch·ªçn:** Tinh ch·ªânh c√°ch t√¨m ki·∫øm ƒë·ªÉ c√≥ k·∫øt qu·∫£ ch√≠nh x√°c h∆°n.
            5.  Nh·∫•n n√∫t **T√¨m ki·∫øm** ƒë·ªÉ xem k·∫øt qu·∫£.
            """)
        
        if reset_submitted:
            st.session_state.search_results = []
            st.session_state.last_keyword = ""
            
        if search_submitted and keyword:
            # S·ª¨A L·ªñI 2: L∆∞u l·∫°i T·∫§T C·∫¢ th√¥ng s·ªë t√¨m ki·∫øm v√†o session_state
            st.session_state.last_keyword = keyword
            st.session_state.last_remove_diac = remove_diac
            
            docs_to_process = []
            if selected_doc_title == "T·∫•t c·∫£ vƒÉn b·∫£n":
                if manifest: docs_to_process.extend(manifest)
            elif manifest:
                docs_to_process.append(title_to_item_map[selected_doc_title])
            
            chunks_all = []
            with st.spinner(f"ƒêang x·ª≠ l√Ω {len(docs_to_process) + len(uploaded_files)} file..."):
                for item in docs_to_process:
                    try:
                        pdf_bytes = download_pdf(item["url"])
                        pages = pdf_to_pages(pdf_bytes)
                        chunks = build_chunks(pages, item.get('title', item['name']), item["url"], chunk_size, overlap)
                        chunks_all.extend(chunks)
                    except Exception as e:
                        st.error(f"L·ªói v·ªõi file {item['name']}: {e}")

                for uploaded_file in uploaded_files:
                    try:
                        pdf_bytes = uploaded_file.getvalue()
                        pages = pdf_to_pages(pdf_bytes)
                        chunks = build_chunks(pages, uploaded_file.name, None, chunk_size, overlap)
                        chunks_all.extend(chunks)
                    except Exception as e:
                        st.error(f"L·ªói v·ªõi file t·∫£i l√™n {uploaded_file.name}: {e}")

                if chunks_all:
                    vec, X = make_index(tuple(chunks_all))
                    results = search(keyword, chunks_all, vec, X, remove_diac, exact_match, top_k)
                    st.session_state.search_results = results
                    st.rerun()
                else:
                    st.warning("Kh√¥ng c√≥ vƒÉn b·∫£n n√†o ƒë∆∞·ª£c x·ª≠ l√Ω.")
                    st.session_state.search_results = []
            
            if not st.session_state.search_results:
                 st.warning("Kh√¥ng t√¨m th·∫•y k·∫øt qu·∫£ ph√π h·ª£p.")

    else: # Khi c√≥ k·∫øt qu·∫£ ƒë·ªÉ hi·ªÉn th·ªã
        if st.button("‚Ü©Ô∏è Tr·ªü v·ªÅ giao di·ªán tra c·ª©u"):
            st.session_state.search_results = []
            st.session_state.last_keyword = ""
            st.rerun()

        results = st.session_state.search_results
        st.markdown("---")
        st.subheader(f"T√¨m th·∫•y {len(results)} k·∫øt qu·∫£ cho t·ª´ kh√≥a \"{st.session_state.last_keyword}\":")
        for sort_score, raw_score, ch in results:
            # S·ª¨A L·ªñI 3: Lu√¥n ƒë·ªçc c√°c tham s·ªë t·ª´ session_state ƒë·ªÉ highlight
            snippet = highlight(ch.text, st.session_state.last_keyword, st.session_state.last_remove_diac)
            with st.container(border=True):
                source_info = f"**T√†i li·ªáu:** `{ch.doc_name}` ¬∑ **Trang:** {ch.page} ¬∑ **M·ª©c ƒë·ªô ph√π h·ª£p:** {raw_score * 100:.1f}%"
                if ch.url:
                    source_info += f"\n\n**Ngu·ªìn:** [M·ªü PDF t·∫°i trang {ch.page}]({ch.url}#page={ch.page})"
                else:
                    source_info += "\n\n**Ngu·ªìn:** T·ªáp ƒë√£ t·∫£i l√™n"
                st.markdown(source_info, unsafe_allow_html=True)
                st.markdown(f"> {snippet}", unsafe_allow_html=True)

# --- N·ªôi dung cho Tab 2 ---
with tab_bieu_mau:
    st.header("Quy tr√¨nh x√°c nh·∫≠n sinh vi√™n online")
    st.markdown("""
    - **H∆∞·ªõng d·∫´n x√°c nh·∫≠n sinh vi√™n tr∆∞·ªùng ƒë·∫°i h·ªçc th·ªßy l·ª£i theo h√¨nh th·ª©c online:** [Link](https://tlu.edu.vn/quy-trinh-xac-nhan-sinh-vien-online-cua-truong-dai-hoc-thuy-loi-42898/)
    """)    
    st.header("Danh s√°ch c√°c bi·ªÉu m·∫´u")
    st.markdown("""
    - **C√°c m·∫´u ƒë∆°n cho Sinh vi√™n:** [Link](https://tlu.edu.vn/cac-mau-don-cho-sinh-vien-36463/)
    """)