# pages/4_H·ªèi_ƒë√°p.py
import streamlit as st
from supabase import create_client, Client
from datetime import datetime
import requests

# --- TH√äM M·ªöI: Import c√°c th∆∞ vi·ªán cho RAG ---
from langchain_community.document_loaders import PyPDFLoader
from langchain.text_splitter import RecursiveCharacterTextSplitter
from langchain_community.embeddings import HuggingFaceEmbeddings
from langchain_community.vectorstores import FAISS
from langchain_google_genai import ChatGoogleGenerativeAI
from langchain.chains.question_answering import load_qa_chain
from langchain.prompts import PromptTemplate
import tempfile
import os
       
# --- 1. K·∫æT N·ªêI V·ªöI SUPABASE & GOOGLE (C·∫•u h√¨nh cho Streamlit Cloud) ---
MANIFEST_URL_DEFAULT = "https://raw.githubusercontent.com/thanvanvan82/cvht-streamlit-app/main/manifest.json"

@st.cache_resource
def init_supabase_connection():
    """Kh·ªüi t·∫°o k·∫øt n·ªëi Supabase v·ªõi error handling cho Streamlit Cloud"""
    try:
        # Ki·ªÉm tra xem c√≥ secrets kh√¥ng
        if "SUPABASE_URL" not in st.secrets:
            st.warning("‚ö†Ô∏è SUPABASE_URL ch∆∞a ƒë∆∞·ª£c c·∫•u h√¨nh trong Streamlit Cloud secrets")
            return None
            
        if "SUPABASE_KEY" not in st.secrets:
            st.warning("‚ö†Ô∏è SUPABASE_KEY ch∆∞a ƒë∆∞·ª£c c·∫•u h√¨nh trong Streamlit Cloud secrets")
            return None
            
        url = st.secrets["SUPABASE_URL"]
        key = st.secrets["SUPABASE_KEY"]
        
        # Import supabase client (ƒë·∫£m b·∫£o ƒë√£ c√†i trong requirements.txt)
        from supabase import create_client, Client
        
        supabase_client = create_client(url, key)
        #st.success("‚úÖ K·∫øt n·ªëi Supabase th√†nh c√¥ng!")
        return supabase_client
        
    except ImportError:
        st.error("‚ùå Thi·∫øu package 'supabase'. Th√™m 'supabase' v√†o requirements.txt")
        return None
    except Exception as e:
        st.error(f"‚ùå L·ªói k·∫øt n·ªëi Supabase: {e}")
        st.info("üí° H∆∞·ªõng d·∫´n: V√†o Settings > Secrets trong Streamlit Cloud ƒë·ªÉ th√™m SUPABASE_URL v√† SUPABASE_KEY")
        return None

# Kh·ªüi t·∫°o Supabase client
supabase: Client = init_supabase_connection()

# C·∫•u h√¨nh Google API
def init_google_api():
    """Kh·ªüi t·∫°o Google API v·ªõi error handling cho Streamlit Cloud"""
    try:
        if "GOOGLE_API_KEY" not in st.secrets:
            st.error("‚ùå GOOGLE_API_KEY ch∆∞a ƒë∆∞·ª£c c·∫•u h√¨nh trong Streamlit Cloud secrets!")
            st.info("üí° V√†o Settings > Secrets ƒë·ªÉ th√™m GOOGLE_API_KEY")
            st.stop()
        
        google_api_key = st.secrets["GOOGLE_API_KEY"]
        
        # Import v√† c·∫•u h√¨nh Google Generative AI
        import google.generativeai as genai
        genai.configure(api_key=google_api_key)
        
        #st.success("‚úÖ Google API ƒë√£ ƒë∆∞·ª£c c·∫•u h√¨nh!")
        return google_api_key
        
    except ImportError:
        st.error("‚ùå Thi·∫øu package 'google-generativeai'. Ki·ªÉm tra requirements.txt")
        st.stop()
    except Exception as e:
        st.error(f"‚ùå L·ªói c·∫•u h√¨nh Google API: {e}")
        st.stop()

# Kh·ªüi t·∫°o Google API
GOOGLE_API_KEY = init_google_api()
          
# --- 2. C√ÅC H√ÄM C≈® (Qu·∫£n l√Ω FAQ v√† L·ªãch s·ª≠) ---
@st.cache_data(ttl=600)
def get_faqs():
    if not supabase: return []
    try:
        response = supabase.table("cau_hoi_thuong_gap").select("*").not_.is_("tra_loi", "null").order("luot_hoi", desc=True).execute()
        return response.data
    except Exception as e:
        st.error(f"L·ªói khi t·∫£i c√¢u h·ªèi: {e}")
        return []

def submit_new_question_to_faq(user_info, new_question):
    if not supabase: return
    try:
        normalized_question = new_question.strip().lower()
        existing_q = supabase.table("cau_hoi_thuong_gap").select("id, luot_hoi").eq("cau_hoi", normalized_question).execute()

        if existing_q.data:
            q_id = existing_q.data[0]['id']
            new_count = existing_q.data[0]['luot_hoi'] + 1
            supabase.table("cau_hoi_thuong_gap").update({"luot_hoi": new_count}).eq("id", q_id).execute()
        else:
            supabase.table("cau_hoi_thuong_gap").insert({"cau_hoi": normalized_question, "luot_hoi": 1}).execute()
        
        log_submission(user_info, f"[FAQ Submission] {new_question}")
        
    except Exception as e:
        st.error(f"ƒê√£ c√≥ l·ªói x·∫£y ra khi g·ª≠i c√¢u h·ªèi FAQ: {e}")

# H√ÄM LOG_SUBMISSION ƒê√É C·∫¨P NH·∫¨T
def log_submission(user_info, question=None, answer=None): # Th√™m tham s·ªë 'answer=None'
    if not supabase: return
    try:
        # Th√™m c·ªôt 'tra_loi_moi' v√†o dictionary
        supabase.table("lich_su_hoi").insert({
            "msv": user_info["msv"], "ho_ten": user_info["ho_ten"], "lop": user_info["lop"],
            "sdt": user_info["sdt"], "email": user_info["email"], "cau_hoi_moi": question,
            "created_at": datetime.now().isoformat(),
            "tra_loi_moi": answer  # D√≤ng m·ªõi ƒë∆∞·ª£c th√™m v√†o
        }).execute()
    except Exception as e:
        st.error(f"L·ªói khi ghi nh·∫≠n th√¥ng tin: {e}")

# --- 3. TH√äM M·ªöI: C√°c h√†m cho RAG (H·ªèi ƒë√°p v·ªõi t√†i li·ªáu) ---
@st.cache_data(show_spinner="ƒêang t·∫£i danh s√°ch t√†i li·ªáu...", ttl=3600)
def fetch_manifest(url):
    try:
        r = requests.get(url, timeout=15)
        r.raise_for_status()
        return r.json()
    except Exception:
        return []

@st.cache_data(show_spinner=False, ttl=3600)
def download_pdf(url):
    r = requests.get(url, timeout=60)
    r.raise_for_status()
    return r.content

@st.cache_resource(show_spinner="ƒêang x·ª≠ l√Ω t√†i li·ªáu ƒë√£ ch·ªçn...")
def create_vector_store_from_manifest(_selected_docs_tuple):
    """
    H√†m n√†y nh·∫≠n m·ªôt tuple (hashable) c·ªßa c√°c t√†i li·ªáu, x·ª≠ l√Ω v√† t·∫°o vector store.
    """
    # Chuy·ªÉn ƒë·ªïi tuple of frozensets tr·ªü l·∫°i th√†nh list of dicts
    selected_docs = [dict(item) for item in _selected_docs_tuple]
    
    # S·ª¨A L·ªñI 1: Thay v√¨ t·∫°o list r·ªóng cho text, ta t·∫°o list r·ªóng cho documents
    all_pages = [] 

    for doc_info in selected_docs:
        try:
            pdf_bytes = download_pdf(doc_info['url'])
            with tempfile.NamedTemporaryFile(delete=False, suffix=".pdf") as tmpfile:
                tmpfile.write(pdf_bytes)
                pdf_path = tmpfile.name
            
            try:
                loader = PyPDFLoader(pdf_path)
                pages = loader.load_and_split()
                # S·ª¨A L·ªñI 2: Th√™m tr·ª±c ti·∫øp c√°c ƒë·ªëi t∆∞·ª£ng Document v√†o danh s√°ch
                all_pages.extend(pages)
            finally:
                os.remove(pdf_path)
        except Exception as e:
            st.warning(f"L·ªói khi x·ª≠ l√Ω file {doc_info.get('title', 'N/A')}: {e}")
            
    if not all_pages:
        return None

    # S·ª¨A L·ªñI 3: S·ª≠ d·ª•ng ph∆∞∆°ng th·ª©c ƒë√∫ng l√† 'split_documents' v·ªõi ƒë·∫ßu v√†o l√† list of Documents
    text_splitter = RecursiveCharacterTextSplitter(chunk_size=1000, chunk_overlap=200)
    split_chunks_as_docs = text_splitter.split_documents(all_pages)
    
    embedding_model_name = "keepitreal/vietnamese-sbert"
    embeddings = HuggingFaceEmbeddings(model_name=embedding_model_name)
    
    # S·ª¨A L·ªñI 4: D√πng FAISS.from_documents thay v√¨ from_texts
    vector_store = FAISS.from_documents(split_chunks_as_docs, embedding=embeddings)
    return vector_store
    
def get_conversational_chain():
    prompt_template = """
    B·∫°n l√† m·ªôt tr·ª£ l√Ω AI chuy√™n nghi·ªáp, nhi·ªám v·ª• c·ªßa b·∫°n l√† tr·∫£ l·ªùi c√¢u h·ªèi ch·ªâ d·ª±a tr√™n n·ªôi dung t√†i li·ªáu ƒë∆∞·ª£c cung c·∫•p.
    H√£y ƒë·ªçc k·ªπ b·ªëi c·∫£nh v√† tr·∫£ l·ªùi c√¢u h·ªèi c·ªßa ng∆∞·ªùi d√πng.
    N·∫øu th√¥ng tin kh√¥ng c√≥ trong b·ªëi c·∫£nh, h√£y n√≥i r√µ "T√¥i kh√¥ng t√¨m th·∫•y th√¥ng tin n√†y trong t√†i li·ªáu ƒë∆∞·ª£c cung c·∫•p."

    B·ªëi c·∫£nh:\n{context}\n
    C√¢u h·ªèi:\n{question}\n

    C√¢u tr·∫£ l·ªùi chi ti·∫øt (b·∫±ng ti·∫øng Vi·ªát):
    """
    model = ChatGoogleGenerativeAI(model="gemini-1.5-flash-latest", temperature=0.3, google_api_key=GOOGLE_API_KEY)
    prompt = PromptTemplate(template=prompt_template, input_variables=["context", "question"])
    chain = load_qa_chain(model, chain_type="stuff", prompt=prompt)
    return chain

# --- 4. GIAO DI·ªÜN STREAMLIT ---
st.title("üôã‚Äç‚ôÇÔ∏è H·ªèi & ƒê√°p d√†nh cho Sinh vi√™n")

if 'user_info_submitted' not in st.session_state:
    st.session_state.user_info_submitted = False
    st.session_state.user_info = {}

if not st.session_state.user_info_submitted:
    st.warning("Vui l√≤ng cung c·∫•p th√¥ng tin c·ªßa b·∫°n ƒë·ªÉ ti·∫øp t·ª•c.")
    with st.form("user_info_form"):
        st.subheader("Th√¥ng tin sinh vi√™n")
        msv = st.text_input("M√£ s·ªë sinh vi√™n (*)")
        ho_ten = st.text_input("H·ªç v√† t√™n (*)")
        lop = st.text_input("L·ªõp (*)")
        sdt = st.text_input("S·ªë ƒëi·ªán tho·∫°i")
        email = st.text_input("Email")
        
        submitted = st.form_submit_button("X√°c nh·∫≠n v√† ti·∫øp t·ª•c")
        if submitted:
            if not msv or not ho_ten or not lop:
                st.error("Vui l√≤ng ƒëi·ªÅn ƒë·∫ßy ƒë·ªß c√°c tr∆∞·ªùng c√≥ d·∫•u (*).")
            else:
                st.session_state.user_info = {"msv": msv, "ho_ten": ho_ten, "lop": lop, "sdt": sdt, "email": email}
                st.session_state.user_info_submitted = True
                log_submission(st.session_state.user_info)
                st.rerun()
else:
    st.success(f"Ch√†o m·ª´ng {st.session_state.user_info['ho_ten']}!")
    
    # --- Tab m·ªõi ƒë·ªÉ t√°ch bi·ªát hai ch·ª©c nƒÉng ---
    tab_rag, tab_faq = st.tabs(["H·ªèi ƒë√°p v·ªõi t√†i li·ªáu", "C√¢u h·ªèi th∆∞·ªùng g·∫∑p"])
    
    with tab_faq:
        st.header("1. C√°c c√¢u h·ªèi th∆∞·ªùng g·∫∑p")
        faqs = get_faqs()
        if not faqs:
            st.info("Hi·ªán ch∆∞a c√≥ c√¢u h·ªèi n√†o ƒë∆∞·ª£c tr·∫£ l·ªùi.")
        else:
            for faq in faqs:
                with st.expander(f"‚ùì {faq['cau_hoi'].capitalize()}"):
                    st.write(faq['tra_loi'])

        st.markdown("---")
        st.header("2. G√≥p √Ω c√¢u h·ªèi cho m·ª•c n√†y")
        st.write("N·∫øu b·∫°n c√≥ m·ªôt c√¢u h·ªèi chung m√† b·∫°n nghƒ© n√™n c√≥ trong m·ª•c tr√™n, h√£y g·ª≠i n√≥ ·ªü ƒë√¢y.")
        with st.form("new_faq_question_form"):
            new_question_text = st.text_area("Nh·∫≠p c√¢u h·ªèi c·ªßa b·∫°n:")
            submit_q_button = st.form_submit_button("G·ª≠i c√¢u h·ªèi")
            if submit_q_button and new_question_text:
                submit_new_question_to_faq(st.session_state.user_info, new_question_text)
                st.success("G·ª≠i g√≥p √Ω th√†nh c√¥ng! C·∫£m ∆°n b·∫°n.")
            elif submit_q_button:
                st.warning("Vui l√≤ng nh·∫≠p c√¢u h·ªèi.")

    with tab_rag:
        st.header("H·ªèi ƒë√°p d·ª±a tr√™n c√°c vƒÉn b·∫£n, quy ch·∫ø")
        st.write("Ch·ªçn c√°c t√†i li·ªáu b·∫°n mu·ªën h·ªèi, sau ƒë√≥ ƒë·∫∑t c√¢u h·ªèi. AI s·∫Ω tr·∫£ l·ªùi d·ª±a tr√™n n·ªôi dung b·∫°n ƒë√£ ch·ªçn.")
        
        manifest = fetch_manifest(MANIFEST_URL_DEFAULT)
        if manifest:
            doc_options = {item.get('title', item['name']): item for item in manifest}
            selected_titles = st.multiselect(
                "Ch·ªçn m·ªôt ho·∫∑c nhi·ªÅu t√†i li·ªáu ƒë·ªÉ h·ªèi:",
                options=list(doc_options.keys()),
                default=[list(doc_options.keys())[0]] if doc_options else [] # M·∫∑c ƒë·ªãnh ch·ªçn t√†i li·ªáu ƒë·∫ßu ti√™n
            )
            
            if selected_titles:
                selected_docs_info = [doc_options[title] for title in selected_titles]
                # Chuy·ªÉn list of dicts th√†nh tuple of frozensets ƒë·ªÉ c√≥ th·ªÉ cache
                hashable_docs_info = tuple(frozenset(item.items()) for item in selected_docs_info)
                vector_store = create_vector_store_from_manifest(hashable_docs_info)

                if 'rag_messages' not in st.session_state:
                    st.session_state.rag_messages = []

                for message in st.session_state.rag_messages:
                    with st.chat_message(message["role"]):
                        st.markdown(message["content"])

                if user_question := st.chat_input("ƒê·∫∑t c√¢u h·ªèi v·ªÅ t√†i li·ªáu ƒë√£ ch·ªçn..."):
                    if vector_store:
                        st.session_state.rag_messages.append({"role": "user", "content": user_question})
                        with st.chat_message("user"):
                            st.markdown(user_question)
                        
                        with st.chat_message("assistant"):
                            with st.spinner("ƒêang t√¨m ki·∫øm trong t√†i li·ªáu..."):
                                conversation_chain = get_conversational_chain()
                                docs = vector_store.similarity_search(user_question)
                                response = conversation_chain({"input_documents": docs, "question": user_question}, return_only_outputs=True)
                                answer = response["output_text"]
                                st.markdown(answer)
                                st.session_state.rag_messages.append({"role": "assistant", "content": answer})
                                log_submission(st.session_state.user_info, f"[RAG] {user_question}", answer)
                    else:
                        st.warning("Kh√¥ng th·ªÉ x·ª≠ l√Ω c√°c t√†i li·ªáu ƒë√£ ch·ªçn. C√≥ th·ªÉ file b·ªã l·ªói ho·∫∑c tr·ªëng.")
            else:
                st.warning("Vui l√≤ng ch·ªçn √≠t nh·∫•t m·ªôt t√†i li·ªáu ƒë·ªÉ b·∫Øt ƒë·∫ßu h·ªèi ƒë√°p.")
        else:
            st.error("Kh√¥ng th·ªÉ t·∫£i danh s√°ch t√†i li·ªáu t·ª´ manifest.")
